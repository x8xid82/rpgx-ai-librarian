## License

Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) License

This work is licensed under the Creative Commons Attribution-NonCommercial 4.0
International License. To view a copy of this license, visit
http://creativecommons.org/licenses/by-nc/4.0/.

### Summary of License Terms:

* **Attribution**: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.
* **NonCommercial**: You may not use the material for commercial purposes.

For more information, please see the full text of the license at
http://creativecommons.org/licenses/by-nc/4.0/legalcode

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

---

## Contributors

* Ashton Rogers (@x8xid82)
* RPGX Studios
* X8 Studios

---

# ğŸ§  RPGX-AI Library (v1.3)

*A Retrieval-Augmented Knowledge System for the RPGX-AI Assistant Module*

---

## Overview

**RPGX-AI Library** transforms RPGX-AI into a context-aware assistant for Game Masters and world-builders.  
It allows you to **ingest journals, notes, NPC sheets, and (soon) compendiums** into a locally running AI â€œknowledge base.â€  
The AI can then answer questions, explain rules, summarize lore, or recall information directly from your game world.

Instead of relying solely on a language modelâ€™s generic memory, the module builds a searchable **Retrieval-Augmented Generation (RAG)** database that references your own Foundry content.  
When you ask the AI something, it first retrieves the most relevant chunks of your uploaded documents, and then uses them to generate an answerâ€”citing its sources.

---

## âš™ï¸ How It Works

1. **Data Ingestion**

   * The module sends the plain text of your Foundry journals (and soon other document types) to a lightweight local RAG server.
   * The server splits your text into manageable â€œchunksâ€ and generates vector embeddings using **Ollama** and your chosen embedding model (e.g., `nomic-embed-text`).
   * These vectors are stored in `kb.json`, forming your knowledge base.

2. **Question-Answering**

   * When you query the AI, the RAG server compares your questionâ€™s embedding to all stored chunks.
   * The most relevant chunks are included as context in a prompt to your selected **local LLM** (e.g., `qwen2.5:14b`).
   * The AI then generates an answer using that context, citing the journal(s) it pulled from.

3. **Fallback Behavior**

   * If no relevant context is found, the system gracefully falls back to the modelâ€™s general knowledge baseâ€”ensuring consistent answers even when your RAG data doesnâ€™t match.

---

## ğŸ§© Core Features

* ğŸ—‚ï¸ **Journal Ingestion:** Add all or individual journals into your AIâ€™s knowledge base.
* ğŸ§¾ **Full Text Parsing:** Automatically extracts text content from journal pages (HTML, Markdown, legacy).
* ğŸ§¹ **Knowledge Wipe:** Reset or clear the stored data from the RAG server.
* âš™ï¸ **Configurable Settings:**

  * **Chunk Size / Overlap** â€” Controls text segmentation granularity.
  * **RAG Top-K** â€” Number of chunks retrieved per query.
  * **Temperature / Max Tokens / Timeout** â€” AI generation settings for response creativity and performance.
  * **System Prompt (Persona)** â€” Define your AIâ€™s tone and behavior.

* ğŸ’¬ **Smart Citation:** AI cites which Foundry journals were used in its answer.
* ğŸ§  **Local \& Private:** All processing happens on your own machineâ€”no external API or cloud dependencies.
* ğŸ§© **Extensible:** Future updates will add support for **Actors**, **Compendiums**, and custom ingest types.

---

## ğŸ–¥ï¸ System Requirements

* **Foundry VTT:** Version **v12** or newer
* **Node.js:** Version **18+** (for the RAG server)
* **Ollama:** Installed locally with at least one supported model

  * Recommended LLM: `qwen2.5:14b`
  * Recommended Embedding Model: `nomic-embed-text`

* **Operating Systems:** Windows 10/11, macOS, or Linux
* **Hardware:**

  * Minimum: 8 GB RAM, 4 CPU cores
  * Recommended: 16 GB+ RAM and GPU acceleration (CUDA or Metal)

---

## ğŸ“¦ Software Dependencies

| Component | Description |
|------------|-------------|
| \*\*Foundry Module:\*\* `rpgx-ai-assistant` | RPGX-AI \*\*Ollama\*\* integration into FoundryVTT Chat |
| \*\*Foundry Module:\*\* `rpgx-ai-librarian` | Provides ingestion UI and AI integration. |
| \*\*Local RAG Server:\*\* `server.js` | Manages embeddings and retrieval. |
| \*\*Ollama\*\* | Hosts LLM and embedding models. |
| \*\*NPM Packages:\*\* | `express`, `body-parser`, `cors`, `fs-extra`, `http` |

---

## ğŸ§¾ Changelog

### v1.0 â€” Stable Release

**Core Functionality Complete**

* Fully working RAG integration between Foundry VTT and a local Ollama instance.
* Added seamless fallback to core model knowledge when RAG has no context.
* Clean citation display â€” inline journal references in AI responses.
* Refined server.js handling with proper JSON safety and CORS compatibility.
* Implemented reliable ping, ingest, and wipe endpoints.

**Librarian Module Features**

* Added full ingest system for all Foundry journals.
* Added progress notifications and feedback for ingest actions.
* Rebuilt Librarian control panel with working â€œIngest Allâ€ and â€œWipe Knowledge Baseâ€ controls.
* Optimized journal parsing for Foundry v13 compatibility (handles text, HTML, and markdown).
* Improved safety fallbacks and performance tuning.

**Quality of Life Improvements**

* Streamlined error handling and notification messages.
* Ensured fallback to core Ollama model when RAG is unavailable.
* Cleaned up chat card output â€” removed duplicate or broken citation footers.
* Improved logging and debug output for easier troubleshooting.

**Upcoming Features**

* Right-click â€œIngest to RAGâ€ context menu for journals.
* Actor and NPC ingestion support.
* Compendium ingestion with progress feedback.
* Editable system prompt for AI personality customization.

---

ğŸ‰ *RPGX-AI Library v1.0 represents the first fully stable build â€” integrating local AI, Foundry journals, and real-time context retrieval into a single seamless workflow.*

---

## ğŸ› ï¸ Installation Guide

### Step 1 â€” Install Ollama

1. Download from [https://ollama.ai](https://ollama.ai)
2. Run the following commands:

```bash
   ollama pull qwen2.5:14b
   ollama pull nomic-embed-text

### Step 2 â€” Set Up the RAG Server

### Step 2 â€” Set Up the RAG Server

Place server.js and package.json inside a folder, e.g.:

Documents\\rag-server



Open PowerShell or Terminal, then run:

cd $env:USERPROFILE\\Documents\\rag-server
npm install
$env:OLLAMA\_BASE="http://127.0.0.1:11434"
$env:OLLAMA\_LLM="qwen2.5:14b"
$env:OLLAMA\_EMBED="nomic-embed-text"
$env:PORT="3033"
npm start



Confirm itâ€™s running â€” you should see:

RAG server listening on http://127.0.0.1:3033

### Step 3 â€” Install the Foundry Module

Add the module to Foundry via Manifest URL or manual copy into Data/modules.

Enable RPGX-AI Librarian in your Game World.

Configure under Settings â†’ Module Settings â†’ RPGX AI Librarian:

Set the RAG Base URL (default: http://127.0.0.1:3033).

Adjust Chunk Size, Overlap, and Model settings if desired.

\###Step 4 â€” Ingest Journals

Open the Librarian Control Panel:
Settings â†’ RPGX AI Librarian â†’ Open Control Panel

Click â€œIngest All Journalsâ€ or use the header button on a specific Journal.

Watch the console for confirmation:

Uploaded â€œLore of Elturelâ€ (7 chunks)

\###Step 5 â€” Ask Questions

Use any integrated AI interface (e.g., After Dark AI or GM console) to ask questions.
The model will retrieve relevant information from your ingested journals before answering.

---

ğŸ§¹ Maintenance

Wipe Knowledge Base: Clears kb.json and resets the index.

Manual Reset: You can also manually delete kb.json from the RAG folder.

Re-Index: After editing journals, re-ingest them to refresh the knowledge base.

ğŸ§© Future Enhancements

âœ… Right-click context menu for per-journal ingestion

âœ… Editable system prompt for user-defined AI persona

ğŸ”œ Actor and NPC sheet ingestion

ğŸ”œ Compendium ingestion with progress tracking

ğŸ”œ Multi-model configuration and performance benchmarking

ğŸ“š Example Use Cases

â€œSummarize all lore about Elturel from my journals.â€

â€œList the stat blocks for NPCs in the Order of the Veil.â€

â€œExplain how teleportation circles work in my worldâ€™s custom magic system.â€

â€œWhat events led to the fall of Elturel according to my notes?â€

ğŸ’¡ Tips for Optimal Performance
Setting	Recommended	Notes
Chunk Size	1000 â€“ 1200	Balanced detail vs speed
Chunk Overlap	150 â€“ 250	Keeps sentences intact
Top K	6 â€“ 8	Enough for context coverage
Temperature	0.3 â€“ 0.5	Factual and consistent output
Max Tokens	600 â€“ 800	Adequate for full responses
Timeout (ms)	60000 â€“ 90000	Depends on hardware speed
ğŸš€ In Short

RPGX-AI Library bridges Foundry and local AI, turning your journals into a private, searchable, conversational database.
No cloud, no subscriptions â€” just your world, your data, your AI.

